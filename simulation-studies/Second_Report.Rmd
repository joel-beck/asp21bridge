---
title: "Bayesian Ridge Regression - Second Report"
author: "Dominik Strache, Nicolai BÃ¤uerle & Joel Beck"
output:
  bookdown::pdf_document2:
    highlight: tango
    toc: FALSE
    number_sections: TRUE
    df_print: tibble
    latex_engine: pdflatex
    keep_tex: FALSE
urlcolor: blue
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  dpi = 300, collapse = TRUE
)
options(knitr.table.format = "latex")
```

```{r, include=FALSE}
pacman::p_load(
  dplyr, readr, ggplot2, tidyr, kableExtra, patchwork, here, magrittr
)
library(asp21bridge)
```



# Introduction {-}

Our first report was focused on the underlying mathematical model of Bayesian Ridge Regression and provided a brief introduction to the `asp21bridge` package. 
Since then, the central `mcmc_ridge()` function, which implements the Markov Chain Monte Carlo sampler and whose name has been adapted to meet existing naming conventions of other groups, has been slightly extended as well as fully unit tested and now shows a stable and reliable performance.

This report investigates the effect of manipulating some of the sampling parameters in a controlled environment, such that changes in the estimation outcome can be directly linked to respective changes in the model inputs.
After an introductory exploration phase, we decided for a subset of all possible model variations that indicated the greatest potential for interesting and relevant findings.

As a result, the simulation studies of this report will be conducted on the following components:

- The **data** input (sections \@ref(corr) and \@ref(outcome)), which is captured by the function argument `m` or alternatively the combination of `X`, `Z` and `y`.
Here, the sampler's *robustness* is tested across various scenarios.

- The **sample size** `n` (section \@ref(n)) and the **number of simulations** `num_sim` (section \@ref(numsim)).
These sections investigate, if a stabilization process happens with increasing either of these two input parameters hinting at *asymptotic*/*convergence* properties.

- The **hyperparameters** `a_tau`, `b_tau`, `a_xi` and `b_xi` (section \@ref(hyper)) of the Inverse Gamma prior distribution of the variance parameters $\tau$ and $\xi$, as specified in the first report.
The effect of hyperparameters in a hierarchical Bayesian model can be difficult to predict based on pure logical reasoning.
Therefore simulations are a useful tool to either confirm prior assumptions or discover unexpected behaviour.

Since the resulting simulation studies serve different purposes (e.g. diagnostic vs. explorative), they demand for different approaches in the simulation settings, the implementation as well as the analysis and presentation of the results.
For that reason, we decided against forcing all of the following sections into one common rigid framework.
Instead, each section individually motivates, explains and interprets the methods chosen for its particular use case.

In order to keep the analysis compact and succinct, there will be almost no code included.
It it worth noting though that the `R Markdown` document generating this report as well as all `R Scripts` used for simulation are included in the `simulation-studies` folder inside the `asp21bridge` package.
Thus, each figure as well as all numerical results are fully reproducible and can be repeated and extended by the reader.








# Correlated Predictor Variables {#corr}

```{r, include=FALSE}
corr_regressors_list <- readr::read_rds(
  file = here::here("simulation-studies", "regressor-correlation.rds")
)
```

Up to this point, we have often illustrated the usage and results of the `mcmc_ridge()` sampler with simulated data from the built-in `toy_data` set.
There, each regressor variable is independently sampled from a normal distribution and the outcome variable is simulated based on a correctly specified location-scale regression model $y_i \sim \mathcal{N}_{} \left( \mathbf{x}^T_i \boldsymbol{\beta}, \exp \left( \mathbf{z}_i^T \boldsymbol{\gamma} \right)^2 \right)$.
All these conditions lead to an excellent performance of the MCMC sampler, but might arguably not represent the most challenging task.

Sections \@ref(corr) and \@ref(outcome) analyze the sampler's performance on simulated data, which might be closer to data found in the real world.
First, we will induce correlation among the predictor variables, whereas in the following section the distributional assumptions are considerably changed.
Further, the `mcmc_ridge()` performance is compared to the Maximum Likelihood based `lmls()` estimator and the Markov Chain Monte Carlo `mcmc()` sampler from the `lmls` package.

## Simulation Setting

- The design matrix $\mathbf{X} = \begin{pmatrix} \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{x}_3 \end{pmatrix}$ is simulated from a three dimensional normal distribution $\mathcal{N}_{3} \left( \boldsymbol{\mu}, \mathbf{\Sigma} \right)$ with mean vector $\boldsymbol{\mu} = \begin{pmatrix} -5 & 2 & 0 \end{pmatrix}^T$ and covariance matrix $\begin{pmatrix} 1 & \rho & \rho \\ \rho & 3 & \rho \\ \rho & \rho & 5 \end{pmatrix}$.
Hence, the dependence among the regressors is fully determined by the parameter $\rho$.

- The design matrix $\mathbf{Z} = \begin{pmatrix} \mathbf{z}_1 & \mathbf{z}_2 \end{pmatrix}$ consists of linear combinations of the regressors $\mathbf{x}_1$ up to $\mathbf{x}_3$, more specifically $\mathbf{ z}_1 = 0.8 \cdot \mathbf{x}_1 + 0.2 \cdot \mathbf{x}_2$ and $\mathbf{ z}_2 = \mathbf{x}_2 - 0.5 \cdot \mathbf{x}_3$.

- In both design matrices intercept columns are added for estimation purposes. 
The true coefficient vectors are given by $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 & \beta_3 \end{pmatrix}^T = \begin{pmatrix} 0 & 3 & -1 & 1 \end{pmatrix}^T$ and $\boldsymbol{\gamma} = \begin{pmatrix} \gamma_0 & \gamma_1 & \gamma_2 \end{pmatrix}^T = \begin{pmatrix} 0 & 2 & 0 \end{pmatrix}^T$.

- Three different values ($0$, $-0.5$ and $0.9$) were chosen for $\rho$ to compare the 'nice' case of uncorrelated predictors with the performance for negative and positive dependence.
For each covariance structure the three models `mcmc_ridge()`, `mcmc()` and `lmls()` were fitted, where each Posterior Mean estimate from both of the Markov Chain Monte Carlo samplers is based on $1000$ samples.

- Moreover, we compared the performance of the usual `mcmc_ridge()` implementation, which draws $\boldsymbol{\beta}$ from the closed form full conditional (multivariate normal) distribution, with an alternative sampling process that uses a Metropolis-Hastings approach for both, the location parameter $\boldsymbol{\beta}$ as well as the scale parameter $\boldsymbol{\gamma}$. 
The latter is initiated by the `mcmc_ridge()` argument `mh_location = TRUE`. 
The variance of the corresponding proposal distribution is set to a carefully chosen default value, but can be manually changed by means of the `prop_var_loc` argument.

## Simulation Results

Figure \@ref(fig:corr-plot-single) displays the posterior mean estimates for the MCMC samplers and the Maximum Likelihood estimates for the `lmls()` function of one complete iteration of the simulation.
For a better visual comparison the true values for each coefficient are indicated by grey circles, whereas the acceptance rate(s) of the Metropolis-Hastings sampling process are provided in grey boxes.

```{r, corr-plot-single, echo=FALSE, out.width="100%", fig.cap="Comparison of Correlation Structures - One Simulation Cycle", fig.pos="t"}
corr_regressors_list$plot_single_sim
```

```{r, corr-plot-many, echo=FALSE, out.width="100%", fig.cap="Comparison of Correlation Structures - 50 Simulation Cycles", fig.pos="t"}
corr_regressors_list$plot_many_sims
```

The scaling of the $x$ - axis is dominated by one outlier in the lower panel for each correlation structure.
While the Metropolis-Hastings approach for $\boldsymbol{\beta}$ performs moderately well for most of the coefficients, it massively overestimates the intercept $\beta_0$.
This observation can be made across many different data sets: In some special cases the performance is close to (but never better) than sampling directly from a multivariate normal distribution, however, most of the time the performance is significantly worse and the samples show (obviously) much larger correlation requiring a higher number of simulations for stable estimation. 
For that reason, we limit the Metropolis-Hastings sampling process for $\boldsymbol{\beta}$ to this one illustration and will focus on the classical `mcmc_ridge()` implementation in the remaining parts of the report.

The upper panel indicates a very good performance by all three estimation procedures in consideration.
Further, all acceptance probabilities are in a reasonable range supporting a fast convergence of all Markov Chains.

It is important to remember, that each point in the plot only represents exactly one measurement.
In order to make any conclusions about bias and variance of the estimation procedures, the above procedure is repeated $50$ times.
The black points represent the mean of these $50$ posterior mean estimates.
Since we cannot rely on distributional theory for the standard errors, the variability of the estimates is displayed by nonparametric 'confidence' intervals, which are simply given by the range from the empirical $0.05$ quantile to the $0.95$ quantile of the $50$ estimated values.

Further investigations have shown that the `mcmc_ridge()`, the `mcmc()` and the `lmls()` functions perform very similar for each correlation structure.
For that reason only the results of the `mcmc_ridge()` sampler are shown in Figure \@ref(fig:corr-plot-many).

There are three conclusions from this first simulation study:

1. The correlation structure does not have a significant impact of the sampling results.
The three plot facets look almost identical.

1. The `mcmc_ridge()` sampler (as well as the `mcmc()` and `lmls()` functions) are very robust towards correlated data and perform extremely well. 
In particular, all three approaches (visually) provide close to unbiased estimates.

1. The variability among the $\boldsymbol{\beta}$ estimates is almost nonexistent, such that results from a single simulation are already reliable and representative.
While the estimates for the $\boldsymbol{\gamma}$ vector are still correct on average, the variability across different simulations is significant (particularly for $\gamma_0$).
Thus, averaging the results from multiple repetitions of the sampling process is advisable.








# Challenging the Model Assumptions {#outcome}

```{r, include=FALSE}
outcome_dist_list <- readr::read_rds(
  file = here::here("simulation-studies", "outcome-distribution.rds")
)
```

This simulation study is structured in a very similar way to the study considered in section \@ref(corr).
Instead of varying the correlation structure among the regressors in the underlying data set, both the regressors and the outcome variable $y$ are sampled from distributions that are more challenging for estimation than the normal distribution.

## Simulation Setting

- The design matrix $\mathbf{X} = \begin{pmatrix} \mathbf{1}_{n} & \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{x}_3 & \mathbf{x}_4 \end{pmatrix}$ contains four independently sampled regressor variables plus one intercept column:
    + $\mathbf{x}_1 \stackrel{ iid}{ \sim} \mathcal{N}_{} \left( 5, 16 \right)$
    + $\mathbf{x}_2 \stackrel{ iid}{ \sim} \mathrm{Exp}(5)$
    + $\mathbf{x}_3 \stackrel{ iid}{ \sim} \mathcal{U}_{} \left( \left[ -2, \; 12 \right] \right)$
    + $\mathbf{x}_4 \stackrel{ iid}{ \sim} \mathrm{Ber}(0.3)$

- The design matrix $\mathbf{Z} = \begin{pmatrix} \mathbf{1}_{n} & \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{z}_3 \end{pmatrix}$ contains the additional regressor variable $\mathbf{z}_3 \stackrel{ iid}{ \sim} t_{10}$, which is independently sampled from all other columns.

- The true coefficient vectors are given by $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 & \beta_3 & \beta_4 \end{pmatrix}^T = \begin{pmatrix} 0 & -3 & -1 & -1 & 2 \end{pmatrix}^T$ and $\boldsymbol{\gamma} = \begin{pmatrix} \gamma_0 & \gamma_1 & \gamma_2 & \gamma_3 \end{pmatrix}^T = \begin{pmatrix} 0 & 1 & 2 & 3 \end{pmatrix}^T$.

- Three different specifications for the outcome distribution were chosen:
    + $y_i \sim \mathcal{N}_{} \left( \mu, \sigma^2 \right)$
    + $y_i \sim \mu + \left(\sigma \cdot \sqrt{ \frac{ 3}{ 5}}\right) T$, where $T \sim t_{5}$
    + $y_i \sim \mu + \sigma \cdot U$, where $U \sim \mathcal{U}_{} \left( \left[ 0, \; 1 \right] \right)$
    
In order to isolate the impact of the different shapes of the three probability distributions, the mean $\mu = \mathbf{x}_i^T \boldsymbol{\beta}$ and the variance $\sigma^2 = \exp \left( \mathbf{z}_i^T \boldsymbol{\gamma} \right)^2$ are held constant across the models.

Note that the `lmls()`, `mcmc()` and `mcmc_ridge()` models are built upon the assumption $y_i \sim \mathcal{N}_{} \left( \mu, \sigma^2 \right)$.
Hence, we expect all three estimation procedures to perform well under the first outcome specification, which they were designed for.
The remaining two cases analyze the performance in presence of a mild ($t$ distribution) and a moderately strong (uniform distribution) violation of the model assumptions.

## Simulation Results

```{r, outcome-plot-single, echo=FALSE, out.width="100%", fig.cap="Comparison of Outcome Distributions - One Simulation Cycle", fig.pos="t"}
outcome_dist_list$plot_single_sim
```

```{r, outcome-plot-many, echo=FALSE, out.width="100%", fig.cap="Comparison of Outcome Distributions - 50 Simulation Cycles", fig.pos="t"}
outcome_dist_list$plot_many_sims
```

Just as in section \@ref(corr) the results of one complete iteration (each of the $3 \cdot 3 = 9$ models was fitted once / each data point represents one estimate) are displayed in Figure \@ref(fig:outcome-plot-single).
Note that the second facet is labeled by $y \sim t$, although it is formally sampled from an affine transformation of a $t$ distributed random variable, which does not follow an exact $t$ distribution.

The differences within each facet as well as between the facets are significant.
All three models seem to estimate the $\boldsymbol{\beta}$ vector well, when there are no or only mild violations of the normal assumption for $y$. 
If $y$ is sampled from a uniform distribution, there are major differences for $\beta_0$, $\beta_2$ and $\beta_4$ (notice the extended $x$-scale in the third facet).
Interestingly, the $\boldsymbol{\gamma}$ vector is estimated very well in the latter case with more deviations in the setting, where $y$ is based on the $t$ distribution.

To gain insights beyond this single simulation cycle, which could well be disturbed by random noise, we repeat the sampling process $50$ times.
The resulting means as well as empirical confidence intervals (analogous to section \@ref(corr)) are plotted in Figure \@ref(fig:outcome-plot-many).

This plot (literally) paints a drastically different picture, emphasizing the necessity of repeating experiments multiple times whenever possible.
Across all $50$ simulations the deviation of the estimates for $\beta_2$ (corresponding to the regressor variable from the exponential distribution) is huge for all three distributional specifications of $y$.
The small bias induced by all three models is negligible compared to the wide confidence intervals, which is particularly interesting when $y$ stems from a normal distribution. 
In this case all models should perform well, however the `lmls()` and the `mcmc()` Posterior Mean / Maximum Likelihood estimates vary wildly across the simulation cycles.
A similar effect can be observed for $\beta_0$ in case of the uniform distribution.
Here, all models overestimate the true value on average, while the `mcmc_ridge()` function again shows the smallest variability.

In contrast, the estimates for $\gamma_0$ in the right facet are fairly stable across simulation cycles, but also consistently wrong at the same time.
Estimates of the bias and the standard error of the Posterior Mean / Maximum Likelihood estimates can be more distinctly compared by their numerical values provided in Tables \@ref(tab:bias) and \@ref(tab:se).
In order to emphasize the interesting/differing entries, both tables only include a subset of the estimated coefficients.

```{r, bias, echo=FALSE}
outcome_dist_list$data_many_sims %>%
  filter(Parameter %in% c("beta_0", "beta_2", "beta_4", "gamma_0", "gamma_2")) %>%
  select(-contains("se")) %>%
  pivot_longer(cols = contains("bias")) %>%
  pivot_wider(names_from = c(outcome_dist, name), values_from = value) %>%
  mutate(Parameter = stringr::str_glue("$\\{Parameter}$")) %>%
  tibble::column_to_rownames(var = "Parameter") %>%
  magrittr::set_colnames(rep(c("lmls", "mcmc", "mcmc\\_ridge"), 3)) %>%
  kableExtra::kbl(
    digits = 2, align = "c", booktabs = TRUE, escape = FALSE,
    caption = "Bias of Coefficient Estimates"
  ) %>%
  kableExtra::kable_styling(
    latex_options = "HOLD_position", position = "center", full_width = FALSE
  ) %>%
  kableExtra::add_header_above(
    header = c("", "Normal" = 3, "t" = 3, "Uniform" = 3)
  )
```

Considering the bias estimates first, there are no obvious patterns that would suggest the superiority of one model.
Further, none of the three models tend to only over- or underestimate the true coefficient values.
The most interesting entries are the bias estimates for $\beta_0$ and $\gamma_0$ in the uniform case, where all three models agree to significantly overestimate.
However, the intercept coefficients are often of minor interest.

```{r, se, echo=FALSE}
outcome_dist_list$data_many_sims %>%
  filter(Parameter %in% c("beta_0", "beta_2", "beta_4", "gamma_0", "gamma_2")) %>%
  select(-contains("bias")) %>%
  pivot_longer(cols = contains("se")) %>%
  pivot_wider(names_from = c(outcome_dist, name), values_from = value) %>%
  mutate(Parameter = stringr::str_glue("$\\{Parameter}$")) %>%
  tibble::column_to_rownames(var = "Parameter") %>%
  magrittr::set_colnames(rep(c("lmls", "mcmc", "mcmc\\_ridge"), 3)) %>%
  kableExtra::kbl(
    digits = 2, align = "c", booktabs = TRUE, escape = FALSE,
    caption = "Standard Errors of Coefficient Estimates"
  ) %>%
  kableExtra::kable_styling(
    latex_options = "HOLD_position", position = "center", full_width = FALSE
  ) %>%
  kableExtra::add_header_above(
    header = c("", "Normal" = 3, "t" = 3, "Uniform" = 3), escape = FALSE
  )
```

The standard error estimates displayed in Table \@ref(tab:se) clearly indicate the worst performance of the `lmls()` and the `mcmc()` model for $\beta_2$.

In almost all cases (and sometimes very significantly), the `mcmc_ridge()` sampler has the smallest standard error.
This finding nicely confirms the underlying mathematical theory:
The present prior specifications in the Bayesian setting, which induces the equivalent form of a frequentist Ridge penalty, can lead to biased estimation.

However, this loss in accuracy can be (as it is in this case) dominated by the gain in precision by the shrinkage effect of the penalty.
Note that (except for $\gamma_0$ in the most right facet in Figure \@ref(fig:outcome-plot-many)) the `mcmc_ridge()` sampler slightly *overestimates* coefficients with true *negative* values and *underestimates* those with true *positive* values. 
This again is caused by the Ridge penalty leading to estimated coefficients close to zero.

In summary, the following conclusions can be drawn:

1. All three models are affected by changes in the regressor and/or outcome distributions.
In the former case the regressor variables sampled from the Exponential and the Bernoulli distributions were the greatest challenge, in the latter case the outcome variable from the Uniform distribution.
This is generally not surprising, since these distributions deviate most from the nicely behaved normally distributed case.

1. As expected, the `lmls()` function is affected strongly by violating the model assumptions, since its estimation process is based on the normal likelihood.
Surprisingly, the `mcmc()` sampler without penalty often did not perform much better.

1. While the `mcmc_ridge()` function does not excel at estimation accuracy, it does lead to the most stable estimation with smallest standard errors in the vast majority of cases.
As emphasized above, this behaviour nicely agrees with the mathematical theory of Ridge penalization.

1. Had we not conducted repeated experiments, our conclusions would have been quite different.
Simulation results are therefore always worth repeating many times to consolidate the correct interpretation.



## Technical Aspects

As outlined in the previous paragraph, a total of $50 \cdot 3 \cdot 3 = 450$ models were fitted to analyze the performance differences.
In order to speed up the involved computations of this specific and some of the other simulation studies in this report, we used the *parallel computing* capabilities of `R`.

There are many options from various packages to choose from.
We decided to use the [furrr](https://furrr.futureverse.org/) package, which is built on top of the `future` package specialized on parallel processing.
As the name suggests, `furrr` provides a convenient way to use many functions from the popular `purrr` package, while using multiple cores at the same time. 
This *functional programming* based approach (similar to the `apply()` family in 'base `R`') is particularly well suited for simulation studies and provides some structural as well as minor performance advantages compared to the classical `for`-loop approach.

The following (slightly modified) code snippet provides a brief insight into the implementation:

```{r, eval=FALSE}
plan(multisession, workers = 8)

full_results <- tibble(id = 1:50) %>%
  mutate(samples = future_map(
    .x = id,
    .f = ~ show_results(n = 50, num_sim = 1000),
    .options = furrr_options(seed = 1)
  ))
```

The `plan()` function borrowed from the `future` package initializes the parallel computing process and the number of cores/workers available for computation.
The `show_results()` helper function fits all three models `mcmc_ridge()`, `mcmc()` and `lmls()` for each outcome distribution in a single simulation cycle.

This entire procedure is repeated $50$ times in parallel using the `future_map()` function from the `furrr` package, where the results of all $450$ models are saved in a well organized structure inside of a list column. 
This new column of the data frame contains complete information about all simulations, such that any required element for the further analysis can be easily extracted and post processed.

Finally, the `.options()` argument allows the specification of a random seed.
Random number generation in the context of parallel computing is slightly more involved than in the sequential approach.
This additional complexity is automatically handled by the `future_map()` function, such that all results are sampled in a statistically valid and fully reproducible manner.














# Sample Size {#n}

This simulation study analyzes the effect of the sample size $n$ on the means of the posterior distribution for the coefficients of $\beta$ and $\gamma$. There are two main goals of this simulation study: On the one hand, we want to investigate whether the posterior means of large samples are closer to the true values than the posterior means of small samples. On the other hand, we want to analyze whether the `mcmc_ridge()` penalty affects the location of the posterior means.

## Simulation Setting

- The design matrix $\mathbf{X} = \begin{pmatrix} \mathbf{1}_{n} & \mathbf{x}_1 & \mathbf{x}_2 \end{pmatrix}$ contains two independently sampled regressor variables plus one intercept column:
    + $\mathbf{x}_1 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{1_n}, \mathbf{I_n} \right)$.
    + $\mathbf{x}_2 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{2*1_n}, \mathbf{I_n} \right)$.

- The design matrix $\mathbf{Z} = \begin{pmatrix} \mathbf{1}_{n} & \mathbf{z}_1 & \mathbf{z}_2 \end{pmatrix}$ contains two independently sampled regressor variables plus one intercept column:
    + $\mathbf{z}_1 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{1_n}, \mathbf{I_n} \right)$.
    + $\mathbf{z}_2 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{2*1_n}, \mathbf{I_n} \right)$.

- The true coefficient vectors are given by $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 \end{pmatrix}^T = \begin{pmatrix} 1 & -1 & 4 \end{pmatrix}^T$ and $\boldsymbol{\gamma} = \begin{pmatrix} \gamma_0 & \gamma_1 & \gamma_2 \end{pmatrix}^T = \begin{pmatrix} 0 & -0.5 & 1 \end{pmatrix}^T$.

- The posterior means are analyzed with respect to 6 different sample sizes: $n = 30, 50, 100, 200, 300, 500.$

- In the next step, the outcome vector $y \in \mathbb{R}^n$ is simulated and passed to the `mcmc_ridge()` function with $nsim = 500$ `mcmc_ridge()` simulations.

- To make the results more stable, above procedure is repeated 100 times. For each coefficient, the mean of the posterior means is calculated as well as the mean absolute error ($MAE$) with respect to the true values of $\beta$ and $\gamma$.

## Simulation Results

```{r, include=FALSE}
plot_data <- readr::read_rds(
  file = here::here("simulation-studies", "samplesize_1.rds")
)
```

```{r, sample-size-means, echo=FALSE, out.width="80%", fig.cap="Mean value of 100 Posterior Mean Estimates", fig.pos="t"}
mean_mean <- plot_data$mean_mean
n_data <- plot_data$n_data

matplot(t(mean_mean),
  x = n_data, type = "b", pch = 1, col = 1:6,
  main = " Mean of Posterior Means",
  xlab = "Sample size", ylab = "", xlim = c(0,700), lty = 2
)
legend(
  x = "right", legend = rownames(mean_mean),
  col = 1:6, pch = 1, bty = "o", title = "Parameter"
)
```

```{r, sample-size-mae, echo=FALSE, out.width="80%", fig.cap="Mean Absolute Error Estimates", fig.pos="t"}
mean_absolute_error <- plot_data$mean_absolute_error

matplot(t(mean_absolute_error),
  x = n_data, type = "b", pch = 1, col = 1:6,
  main = " MAE of Posterior Means",
  xlab = "Sample size", ylab = "", xlim = c(0,700), lty = 2
)
legend(
  x = "right", legend = rownames(mean_absolute_error),
  col = 1:6, pch = 1, bty = "o", title = "Parameter"
)
```

The means of the posterior means are displayed in the Figure \@ref(fig:sample-size-means)
For larger sample sizes ($n \geq 200)$, none of the six parameters are extremely biased.

Moreover, for $n = 30$, $\beta_0$ and $\beta_0$ are significantly biased, which might be caused by the high `mcmc_ridge()` penalty for $\beta_2 = 4$. The significant bias of $\beta_0$ might be explained by a counteract of the $\beta_2$ bias.  

After getting an impression about empirical biases of the coefficients, we now focus on the variability of the posterior means of the coefficients, which are measured by the $MAE$ based on the results of the 100 repetitions.

Figure \@ref(fig:sample-size-mae) points out that the posterior means of $\beta_0$ have significantly larger errors than the posterior means of $\beta_2$ for $n = 30$. However, this might also be explained by the fact that for $n = 30$, $\beta_0$ has a greater empirical bias than $\beta_2$ according to the first plot of this chapter.

In addition, for increasing sample sizes the $MAE$ of the posterior means tend to zero for all coefficients except $\beta_0$. Nevertheless, also the errors of $\beta_0$ seem to become smaller with increasing sample size. 














# Number of Simulations {#numsim}

Another important parameter is $nsim$, the number of Markov Chain Monte Carlo simulations within the `mcmc_ridge()` function.

As explained in our first report, we chose the proposal density for the Metropolis-Hastings algorithm of $\gamma$ in order to achieve acceptance rates between 25% und 50%. A higher number of `mcmc_ridge()` simulations increases the absolute number of accepted proposals and might deliver more precise Posterior distributions, but also deteriorates the speed of the procedure. So there is a trade-off between precision and speed.

This simulation study shall analyze the effect of the $nsim$ parameter on the posterior distribution.

## Simulation Setting

- The design matrix $\mathbf{X} = \begin{pmatrix} \mathbf{1}_{n} & \mathbf{x}_1 & \mathbf{x}_2 \end{pmatrix}$ contains two independently sampled regressor variables plus one intercept column. The sample size $n$ is chosen to be 100.
    + $\mathbf{x}_1 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{1_n}, \mathbf{I_n} \right)$.
    + $\mathbf{x}_2 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{2*1_n}, \mathbf{I_n} \right)$.

- The design matrix $\mathbf{Z} = \begin{pmatrix} \mathbf{1}_{n} & \mathbf{z}_1 & \mathbf{z}_2 \end{pmatrix}$ contains two independently sampled regressor variables plus one intercept column. The sample size $n$ is chosen to be 100.
    + $\mathbf{z}_1 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{1_n}, \mathbf{I_n} \right)$.
    + $\mathbf{z}_2 \stackrel{ iid}{ \sim} \mathcal{N}_{n} \left( \boldsymbol{2*1_n}, \mathbf{I_n} \right)$.

- The true coefficient vectors are given by $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 \end{pmatrix}^T = \begin{pmatrix} 1 & -1 & 4 \end{pmatrix}^T$ and $\boldsymbol{\gamma} = \begin{pmatrix} \gamma_0 & \gamma_1 & \gamma_2 \end{pmatrix}^T = \begin{pmatrix} 0 & -0.5 & 0.5 \end{pmatrix}^T$.

- In the next step, the outcome vector $y \in \mathbb{R}^n$ is simulated and passed to the `mcmc_ridge()` function with $nsim$ `mcmc_ridge()` simulations.

- The posterior means are analyzed with respect to 4 different values for $nsim$: $nsim = 100, 300, 500, 1000.$

- Similar as in the simulation study corcerning the sample size, above procedure is repeated 100 times to make the results more stable. For each coefficient, the mean of the posterior means is calculated as well as the mean absolute error ($MAE$) with respect to the true values of $\beta$ and $\gamma$. In addition, the variance within the Markov Chains are analyzed.

## Simulation Results

```{r, include=FALSE}
num_sim_data <- readr::read_rds(
  file = here::here("simulation-studies", "number_mcmc_ridge_simulations.rds")
)
```

```{r, nsim-mean, echo=FALSE, out.width="80%", fig.cap="Mean value of 100 Posterior Mean Estimates", fig.pos="t"}
mean_mean_num_sim <- num_sim_data$mean_mean
array_num_sim <- num_sim_data$array_num_sim

matplot(t(mean_mean_num_sim),
  x = array_num_sim, type = "b", pch = 1, col = 1:6,
  main = " Mean of Posterior Means",
  xlab = "Number of MCMC Ridge Simulations", ylab = "", xlim = c(0,1300), lty = 2
)
legend(
  x = "topright", legend = rownames(mean_mean_num_sim),
  col = 1:6, pch = 1, bty = "n"
)
```

```{r, nsim-mae, echo=FALSE, out.width="80%", fig.cap="Mean Absolute Error Estimates", fig.pos="t"}
mean_absolute_error_num_sim <- num_sim_data$mean_absolute_error
array_num_sim <- num_sim_data$array_num_sim

matplot(t(mean_absolute_error_num_sim),
  x = array_num_sim, type = "b", pch = 1, col = 1:6,
  main = " MAE of Posterior Means",
  xlab = "Number of MCMC Ridge Simulations", ylab = "", xlim = c(0,1300), lty = 2
)
legend(
  x = "topright", legend = rownames(mean_absolute_error_num_sim),
  col = 1:6, pch = 1, bty = "n"
)
```

```{r, nsim-var, echo=FALSE, out.width="80%", fig.cap="Mean value of 100 Posterior Variance Estimates", fig.pos="t"}
mean_of_variances_within <- num_sim_data$mean_of_variances_within
array_num_sim <- num_sim_data$array_num_sim

matplot(t(mean_of_variances_within),
  x = array_num_sim, type = "b", pch = 1, col = 1:6,
  main = " Mean of Variances within the Samples",
  xlab = "Number of MCMC Ridge Simulations", ylab = "", xlim = c(0,1300), lty = 2
)
legend(
  x = "topright", legend = rownames(mean_of_variances_within),
  col = 1:6, pch = 1, bty = "n"
)
```

Similar as in the sample size simulation study, Figure \@ref(fig:nsim-mean) contains the means of the posterior means for the different values of the $nsim$ parameter.
The result is quite clearly: In the range between $nsim = 100$ and $nsim = 1000$, the $nsim$ parameter has no significant impact on the mean of the posterior means.

Furthermore, for the chosen sample size $n = 100$, none of the 6 parameters has a large bias. 
However, the mean posterior means of $\beta_0$ are all close to 1.07, whereas the mean posterior means of $\beta_2$ are all close to 3.96, such that one can obtain small biases for $\beta_0$ and $\beta_0$, which again might be a result of the high `mcmc_ridge()` penalty for $\beta_2 = 4$ (similar as discussed in section \@ref(n)).

We also analyzed the $MAE$ for different values of $nsim$. 
Figure \@ref(fig:nsim-mae) provides interesting results for the errors of the $\beta$ and $\gamma$ coefficients: 
On the one hand, $nsim$ seems not to have any impact on the errors of the $\beta$ coefficients. 
But on the other hand, an increase of $nsim$ leads to more precise $\gamma$ estimates and apparently lower $MAE$ values for all $\gamma$ coefficients.

In the last step, we analyzed whether the value of $nsim$ has any impact on the variance within the Markov chains.
The results can be obtained in Figure \@ref(fig:nsim-var).
Again, one obtains that the $nsim$ parameter has no impact on the variances within the $\beta$ samples of the Markov chain, whereas the variance within the $\gamma$ samples increases with increasing value of $nsim$. 

This result is not really surprising, because the Markov chain typically jumps to points which are quite close to the previous points. 
As a consequence, if the number of jumps increases, also the variance of the samples might increase.

In combination with the lower $MAE$ for the $\gamma$ parameters, one can conclude that the Posterior distributions of the $\gamma$ parameters might be squeezed for small values of $nsim$ and converge to the true distributions if the number of simulations increases.









# Hyperparameters {#hyper}

```{r, include=FALSE}
hyperparameters_list <- readr::read_rds(
  file = here::here("simulation-studies", "hyperparameters.rds")
)
```

In the past, we have been sampling data by our `mcmc_ridge` function without having a closer look on the effect of the hyperparameters `a_tau`, `b_tau`, `a_xi` and `b_xi`. 
However, they affect the full conditionals of $\tau^2$ and $\xi^2$, which are Inverse Gamma distributed, directly as stated here again for convenience:
<!--  -->
$$
\begin{aligned}
\tau^2 &\mid \cdot \sim IG(a_{\tau} + \frac{ K}{ 2}, \, b_{\tau} + \frac{ 1}{ 2} \tilde{\boldsymbol{\beta}}^T \tilde{\boldsymbol{\beta}}) \\
\xi^2 &\mid \cdot \sim IG(a_{\xi} + \frac{ J}{ 2}, \, b_{\xi} + \frac{ 1}{ 2} \tilde{\boldsymbol{\gamma}}^T \tilde{\boldsymbol{\gamma}}).
\end{aligned}
$$
<!--  -->
In a next step, mean and variance of the full conditional of $\boldsymbol{\beta}$ following a normal distribution are assumed to be positively affected by $\tau^2$:
<!--  -->
$$
\begin{aligned}
\mathbf{\Sigma}_{\beta} = \left( \mathbf{ W}^T \mathbf{ W} + \frac{ 1}{ \tau^2} \left(
    \begin{array}{cc}
      0 & 0 \\
      0 & \mathbf{I}_K \\
    \end{array}
    \right) \right) ^{-1}
\qquad \text{and} \qquad
\boldsymbol{\mu}_{\beta} = \mathbf{\Sigma}_{\beta} \mathbf{ W}^T \mathbf{ u},
\end{aligned}
$$
<!--  -->
with
<!--  -->
$$
\begin{aligned}
\mathbf{ w}_i := \frac{ \mathbf{x}_i}{ \exp \left( \mathbf{z}_i^T \boldsymbol{\gamma} \right)} \in \mathbb{R}^{K+1}, \qquad
\mathbf{ W} := \begin{pmatrix} \mathbf{ w}_1^T \\ \vdots \\ \mathbf{ w}_n^T \end{pmatrix} \in \mathbb{R}^{n \, \times \, (K + 1)}
\end{aligned}
$$
<!--  -->
and
<!--  -->
$$
\begin{aligned}
u_i := \frac{ y_i}{ \exp \left( \mathbf{z}_i^T \boldsymbol{\gamma} \right)} \in \mathbb{R}, \qquad
\mathbf{ u} := \begin{pmatrix} u_1 \\ \vdots \\ u_n \end{pmatrix} \in \mathbb{R}^n,
\end{aligned}
$$
while $\xi^2$ directly affects the full conditional of $\boldsymbol{\gamma}$:
<!--  -->
$$
\begin{aligned}
f( \boldsymbol{\gamma} \mid \cdot )
& \propto \exp \left( - \frac{ 1}{ 2} \cdot \left[ \frac{ 1}{ \xi^2} \tilde{\boldsymbol{\gamma}}^T \tilde{\boldsymbol{\gamma}} + \sum_{i=1}^{n} \left( \frac{ 1}{ \exp \left( \mathbf{ z}_i^T \boldsymbol{\gamma} \right)^2} \left( y_i - \mathbf{x}_i^T \boldsymbol{\beta} \right)^2 + 2 \cdot \mathbf{ z}_i^T \boldsymbol{\gamma} \right) \right] \right) \\
&= \exp \left( - \frac{ 1}{ 2} \cdot \left[ \frac{ 1}{ \xi^2} \tilde{\boldsymbol{\gamma}}^T \tilde{\boldsymbol{\gamma}} + 2 \cdot \mathbf{1}_{n}^T \mathbf{Z} \boldsymbol{\gamma} + \sum_{i=1}^{n} \left( \frac{ y_i - \mathbf{x}_i^T \boldsymbol{\beta}}{ \exp \left( \mathbf{ z}_i^T \boldsymbol{\gamma} \right)} \right)^2 \right] \right).
\end{aligned}
$$
<!--  -->
where $\boldsymbol{\gamma}$ is sampled by a Metropolis Hastings algorithm.
Simulating data samples for different hyperparameter values is therefore inevitable for the whole model. 


## Simulation Setting

- The design matrix $\mathbf{X} = \begin{pmatrix} \mathbf{x}_1 & \mathbf{x}_2 \end{pmatrix}$ is simulated from a two dimensional normal distribution $\mathcal{N}_{2} \left( \boldsymbol{\mu}, \mathbf{\sigma^2 I} \right)$ with mean vector $\boldsymbol{\mu} = \begin{pmatrix} 1 & 2 \end{pmatrix}^T$ and unit variance $\sigma^2_1 = \sigma^2_2 = 1$. 
The same holds true for the design matrix $\mathbf{Z} = \begin{pmatrix} \mathbf{z}_1 & \mathbf{z}_2 \end{pmatrix}$ with mean vector $\boldsymbol{\mu} = \begin{pmatrix} 5 & 3 \end{pmatrix}^T$ also having unit variance.

- In both design matrices intercept columns are added for estimation purposes. 
The true coefficient vectors are given by $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 \end{pmatrix}^T = \begin{pmatrix} 0 & -1 & 4 \end{pmatrix}^T$ and $\boldsymbol{\gamma} = \begin{pmatrix} \gamma_0 & \gamma_1 & \gamma_2 \end{pmatrix}^T = \begin{pmatrix} 0 & -2 & 1 \end{pmatrix}^T$.

- For sampling the location parameter, the full conditional multivariate normal distribution of $\boldsymbol{\beta}$ is chosen, i.e. `mcmc_ridge(..., mh_location = FALSE)` is used. 
Therefore, at least the location estimate is more directly affected by the hyperparameters.

- For simulating the influence of the hyperparameters, nine different values are chosen: $a_{\tau}, b_{_\tau}, a_{\xi}, b_{xi} \in \left\{ -1, 0, 0.5, 1, 2, 10, 50, 100, 200  \right\}$. 
Since for statistical properties like the mean of an Inverse Gamma distribution $\frac{ b}{ a-1}$, the condition $a > 1$ is required, particular attention is focused on larger values. 
Anyway, it is an aim to inspect the performance of the sampler for values less than $1$ as well.

## Simulation Results

```{r, hyppar-beta, echo=FALSE, fig.height=6, fig.cap="Comparison of the absolute deviations of beta parameters", fig.pos="t"}
hyperparameters_list$p1 / hyperparameters_list$p3 / hyperparameters_list$p5 / hyperparameters_list$p7
```

```{r, hyppar-gamma, echo=FALSE, fig.height=6, fig.cap="Comparison of the absolute deviations of gamma parameters", fig.pos="t"}
hyperparameters_list$p2 / hyperparameters_list$p4 / hyperparameters_list$p6 / hyperparameters_list$p8
```

The first two plots of Figures \@ref(fig:hyppar-beta) and \@ref(fig:hyppar-gamma) display the absolute deviations of the posterior mean estimates from the true parameters with the stated different values for $a_{\tau}$ and $b_{\tau}$. 
For each estimate, the posterior mean averages over 1000 simulations of the `mcmc_ridge()` sampler. 
Note, that location and scale parameters are plotted separately, according to the relationship mentioned above. 
For a better overview, the dotted line displays the linear trend of all estimate's deviations.

The scaling of the $x$ - axis is transformed by a pseudo logarithm to clearly visualize the deviations between $-1$ and $10$, which would not be possible on original scales. 
Since $-1$ and $0$ are also part of the hyperparameter values, the `pseudo_log_trans()` of the `scales` package is applied, log-transforming only positive values. 

It can be observed, that the intercept estimates in each plot show the largest deviations from their true value. 
In Figure \@ref(fig:hyppar-beta), however, deviations of $\boldsymbol{\beta}$ estimates are small in absolute value and can be considered to not affect the performance of the sampler unambiguously. 
On the contrary, deviations of the $\boldsymbol{\gamma}$ estimates in Figure \@ref(fig:hyppar-gamma) are absolutely remarkable, especially for $\gamma_0$.

The functional chain that applies to the estimates of $\boldsymbol{\beta}$ can be described by the effect of the mean of the inverse gamma distribution on $\tau^2$. 
A larger value for $b_\tau$ leads to larger values of $\tau^2$, which are again affecting the full posterior parameters of $\boldsymbol{\beta}$ and by this the absolute deviation of the corresponding estimates from their true value to be larger. 
$a_\tau$ causes the opposite effect. 
Nevertheless, this outcome is not clearly visible in the first two plots of Figure \@ref(fig:hyppar-beta) and is assumed to be surpassed by the overall small deviation. 

It is remarkable, that the deviation of ${\boldsymbol{\beta}}$ estimates is the smallest for both, $a_\tau, b_\tau \in \left\{50, 100\right\}$. 
For values of $a_\tau \leq 1$ one obtains wider variances of absolute deviations, since the posterior mean required values larger than one.

In the upper two plots of Figure \@ref(fig:hyppar-gamma), there is no clear impact of $\tau^2$ and its parameters. 
Rooted in no direct effect of $\tau^2$ on $\gamma$ according to our underlying mathematical model, one observes cross-effects through the sampling procedure of our `mcmc_ridge()` sampler, where the full posterior $f( \boldsymbol{\gamma} \mid \cdot )$ depends on $\boldsymbol{\beta}$. 
Anyway, our sampler produces the lowest deviation of ${\boldsymbol{\gamma}}$ estimates  for $a_\tau, b_\tau \in \{0.5, 200\}$, where $0.5$ is chosen by coincidence here, since wide variations for $a_\tau \leq 1$ of absolute deviations are observable again.

The lower two plots of each, Figure \@ref(fig:hyppar-beta) and \@ref(fig:hyppar-gamma) are constructed analogously, but showing the impact of $a_{\xi}$ and $b_{\xi}$ on the location and scale parameters respectively.

Again, absolute deviations for the $\boldsymbol{\beta}$ estimates from their true values are unambiguously small, where these for the $\boldsymbol{\gamma}$ estimates are considerably larger. Once more, also the intercepts display the largest deviations.

Comparably, arguing with the mean of the Inverse Gamma distribution of $\xi^2$, one obtains larger mean values for $b_{\xi}$, while $a_{\xi}$ lowers them. The impact of $\xi^2$ on ${\boldsymbol{\gamma}}$ is assumed to decrease $f( \boldsymbol{\gamma} \mid \cdot )$ according to our underlying model. This effect is indicated by the linear trend lines in the second half of Figure \@ref(fig:hyppar-gamma). In general, one obtains smaller deviations for larger values of $b_{\xi}$ and lower ones of $a_{\xi}$, where especially lots of randomness occurs in the deviations of $\gamma_0$. Therefore, the impact of the $a_{\xi}$ and $b_{\xi}$ on the scale intercepts becoming indistinct through the underlying Metropolis Hastings algorithm. Moreover, the same wide variations only for $a_{\xi} < 1$ cannot be obtained like for $a_{\tau}$

The sampler exhibits the best results for the scale estimates for $a_\xi = 2$, $b_\xi = 100$. In any case, due to the wide overall variation, these results must be taken with care.

The effect of $a_{\xi}$ and $b_{\xi}$ on ${\boldsymbol{\beta}}$ can be explained through the cross-effects of matrix $\mathbf{ W}_i$ and vector $\mathbf{ u}_i$ both containing ${\boldsymbol{\gamma}}$. These diminish with rising ${\boldsymbol{\gamma}}$. $\mathbf{ W}_i$ affecting the variance of the full conditional of $\boldsymbol{\beta}$ negatively, while the mean is positively affected. Hence, larger values of $a_{\xi}$ causing higher posterior means of the location parameters, larger values for $a_{\xi}$ vice versa. Especially, the positive linear trend in the second half of Figure \@ref(fig:hyppar-beta) for values of $a_{\xi}$ is remarkable. For values of $b_{\xi}$, the trend comes off inferior. The wider variations of deviations for $a_{\xi} < 1$ is again not observable here. Nevertheless, the randomness observable for scale estimates does not show up for location estimates anymore.

The smallest deviations of the location estimates are observable for $a_\xi = 1$, and $b_\xi = 200$. 

Shortly noted, the acceptance rates of the Metropolis Hastings algorithm for sampling ${\boldsymbol{\gamma}}$ are always between $0.31$ and $0.53$. For the value range of $a_{\tau}$, $b_{\tau}$ and $a_{\xi}$, no distinct pattern is observable. With growing values of $b_{\xi}$, rates are more likely to grow. Since the acceptance rates are in reasonable ranges for the underlying data and for space-saving reasons, these results are not further revealed here.









# Next Steps {-}

In the process of writing the first and second report, the main work of developing both the mathematical foundation as well as the code base of the `asp21bridge` package has been done.
Thus, the following weeks will focus on filling remaining gaps and working out details that were left open due to time constraints. 
This includes taking a closer look at code efficiency and possible code refactoring for the sake of modularity.

Further, we will reach out to other groups with similar topics (the Bayesian Lasso Regression group in particular) in order to initiate a collaboration. 
In addition to exploring similarities and differences between our own sampler and the procedures from the `lmls` package, this would allow for an interesting comparison between different penalization approaches.

The final step consists of combining all pieces of the project into a single structured and consistent final report.
Aside of the work shown in the first two reports, some new elements like a brief discussion of design and implementation choices during the development stage will be added with the aim of providing the reader a solid understanding of the underlying ideas as well as the practical usage of the package.
