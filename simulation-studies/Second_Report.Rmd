---
title: "Bayesian Ridge Regression - Second Report"
author: "Dominik Strache, Nicolai BÃ¤uerle & Joel Beck"
output:
  bookdown::pdf_document2:
    highlight: tango
    toc: FALSE
    number_sections: TRUE
    df_print: tibble
    latex_engine: pdflatex
    keep_tex: FALSE
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE, fig.align = "center",
  dpi = 300, collapse = TRUE
)
options(knitr.table.format = "latex")
```

```{r, include=FALSE}
pacman::p_load(
  dplyr, readr, ggplot2, tidyr, kableExtra, patchwork, here, magrittr
)
library(asp21bridge)
```

# Introduction {-}

Our first report was focused on the underlying mathematical model of Bayesian Ridge Regression and provided a brief introduction of the `asp21bridge` package. 
Since then, the central `mcmc_ridge()` function, which implements the Markov Chain Monte Carlo sampler and whose name has been adapted to meet implicit naming conventions of other groups, has been slightly extended as well as fully unit tested and now shows a stable and reliable performance.

This report investigates the effect of manipulating some of the sampling parameters in a controlled environment, such that changes in the estimation outcome can be directly linked to respective changes in the model inputs.
After an introductory exploration phase, we decided for a subset of all possible model variations that indicated the greatest potential for interesting and relevant findings.

As a result, the simulation studies of this report will be conducted on the following components:

- The **data** input, which is captured by the function argument `m` or alternatively the combination of `X`, `Z` and `y`.
Here, the sampler's *robustness* is tested across various scenarios.

- The **sample size** `n` and the **number of simulations** `num_sim`.
These sections investigate, if a stabilization process happens with increasing either of these two input parameters hinting at *asymptotic*/*convergence* properties.

- The **hyperparameters** `a_tau`, `b_tau`, `a_xi` and `b_xi` of the Inverse Gamma prior distribution of the variance parameters $\tau$ and $\xi$, as specified in the first report.
The effect of hyperparameters in a hierarchical Bayesian model can be difficult to predict based on pure logical reasoning.
Therefore simulations are a useful tool to either confirm prior assumptions or discover unexpected behaviour.

Since the resulting simulation studies serve different purposes (e.g. diagnostic vs. explorative), they demand for different approaches in the simulation settings, implementation as well as analysis and presentation of the results.
For that reason, we decided against forcing all of the following sections into one common rigid framework.
Instead, each section individually motivates, explains and interprets the methods chosen for its particular use case.

# Correlated Predictor Variables

```{r, include=FALSE}
corr_regressors_list <- readr::read_rds(
  file = here::here("simulation-studies", "regressor-correlation.rds")
)
```

Up to this point, we have often illustrated the usage and results of the `mcmc_ridge()` sampler with simulated data from the built-in `toy_data` set.
There, each regressor variable is independently sampled from a normal distribution and the outcome variable is simulated based on a correctly specified location-scale regression model $y_i \sim \mathcal{N}_{} \left( \mathbf{x}^T_i \boldsymbol{\beta}, \exp \left( \mathbf{z}_i^T \boldsymbol{\gamma} \right)^2 \right)$.
All these conditions lead to an excellent performance of the MCMC sampler, but might arguably not represent the most challenging task.

The next two sections analyze the sampler's performance on simulated data, which might be closer to data found in the real world.
First, we will induce correlation among the predictor variables, whereas in the following section the distributional assumptions are considerably changed.
Further, the `mcmc_ridge()` performance is compared to the Maximum Likelihood based `lmls()` estimator and the Markov Chain Monte Carlo `mcmc()` sampler from the `lmls` package.

## Simulation Setting

- The design matrix $\mathbf{X} = \begin{pmatrix} \mathbf{x}_1 & \mathbf{x}_2 & \mathbf{x}_3 \end{pmatrix}$ is simulated from a three dimensional normal distribution $\mathcal{N}_{3} \left( \boldsymbol{\mu}, \mathbf{\Sigma} \right)$ with mean vector $\boldsymbol{\mu} = \begin{pmatrix} -5 & 2 & 0 \end{pmatrix}^T$ and covariance matrix $\begin{pmatrix} 1 & \rho & \rho \\ \rho & 3 & \rho \\ \rho & \rho & 5 \end{pmatrix}$.
Hence, the dependence among the regressors is fully determined by the parameter $\rho$.

- The design matrix $\mathbf{Z} = \begin{pmatrix} \mathbf{z}_1 & \mathbf{z}_2 \end{pmatrix}$ consists of linear combinations of the regressors $\mathbf{x}_1$ up to $\mathbf{x}_3$, more specifically $\mathbf{ z}_1 = 0.8 \cdot \mathbf{x}_1 + 0.2 \cdot \mathbf{x}_2$ and $\mathbf{ z}_2 = \mathbf{x}_2 - 0.5 \cdot \mathbf{x}_3$.

- In both design matrices intercept columns are added for estimation purposes. 
The true coefficient vectors are given by $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 & \beta_4 \end{pmatrix}^T = \begin{pmatrix} 0 & 3 & -1 & 1 \end{pmatrix}^T$ and $\boldsymbol{\gamma} = \begin{pmatrix} \gamma_0 & \gamma_1 & \gamma_2 \end{pmatrix}^T = \begin{pmatrix} 0 & 2 & 0 \end{pmatrix}^T$.

- Three different values ($0$, $-0.5$ and $0.9$) were chosen for $\rho$ to compare the 'nice' case of uncorrelated predictors with the performance for negative and positive dependence.
For each covariance structure the three models `mcmc_ridge()`, `mcmc()` and `lmls()` were fitted, where each Posterior Mean estimate from both of the Markov Chain Monte Carlo samplers is based on $1000$ samples.

- Moreover, we compared the performance of the usual `mcmc_ridge()` implementation, which draws $\boldsymbol{\beta}$ from the closed form full conditional (multivariate normal) distribution, with an alternative sampling process that uses a Metropolis-Hastings approach for both, the location parameter $\boldsymbol{\beta}$ as well as the scale parameter $\boldsymbol{\gamma}$. 
The latter is initiated by the `mcmc_ridge()` argument `mh_location = TRUE`. 
The variance of the corresponding proposal distribution is set to a carefully chosen default value, but can be manually changed by means of the `prop_var_loc` argument.

## Simulation Results

The following plot displays the posterior mean estimates for the MCMC samplers and the Maximum Likelihood estimates for the `lmls()` function of one complete iteration of the simulation.
For a better visual comparison the true values for each coefficient are indicated by grey circles, whereas the acceptance rate(s) of the Metropolis-Hastings sampling process are provided in grey boxes:

```{r, echo=FALSE, out.width="100%"}
corr_regressors_list$plot_single_sim
```

The scaling of the $x$ - axis is dominated by one outlier in the lower panel for each correlation structure.
While the Metropolis-Hastings approach for $\boldsymbol{\beta}$ performs moderately well for most of the coefficients, it massively overestimates the intercept $\beta_0$.
This observation can be made across many different data sets: In some special cases the performance is close to (but never better) than sampling directly from a multivariate normal distribution, however, most of the time the performance is significantly worse and the samples show (obviously) much larger correlation requiring a higher number of simulations for stable estimation. 
For that reason, we limit the Metropolis-Hastings sampling process for $\boldsymbol{\beta}$ to this one illustration and will focus on the classical `mcmc_ridge()` implementation in the remaining parts of the report.

The upper panel indicates a very good performance by all three estimation procedures in consideration.
Further, all acceptance probabilities are in a reasonable range supporting a fast convergence of all Markov Chains.

It is important to remember, that each point in the plot only represents exactly one measurement.
In order to make any conclusions about bias and variance of the estimation procedures, the above procedure is repeated $50$ times.
The black points represent the mean of these $50$ posterior mean estimates.
Since we cannot rely on distributional theory for the standard errors, the variability of the estimates is displayed by nonparametric 'confidence' intervals, which are simply given by the range from the empirical $0.05$ quantile to the $0.95$ quantile of the $50$ estimated values.

Further investigations have shown that the `mcmc_ridge()`, the `mcmc()` and the `lmls()` functions perform very similar for each correlation structure.
For that reason only the results of the `mcmc_ridge()` sampler are shown in the following plot:

```{r, echo=FALSE, out.width="100%"}
corr_regressors_list$plot_many_sims
```

There are three conclusions from this first simulation study:

1. The correlation structure does not have a significant impact of the sampling results.
The three plot facets look almost identical.

1. The `mcmc_ridge()` sampler (as well as the `mcmc()` and `lmls()` functions) are very robust towards correlated data and perform extremely well. 
In particular, all three approaches (visually) provide close to unbiased estimates.

1. The variability among the $\boldsymbol{\beta}$ estimates is almost nonexistent, such that results from a single simulation are already reliable and representative.
While the estimates for the $\boldsymbol{\gamma}$ vector are still correct on average, the variability across different simulations is significant (particularly for $\gamma_0$).
Thus, averaging the results from multiple repetitions of the sampling process is advisable.



# Challenging the Model Assumptions

```{r, include=FALSE}
outcome_dist_list <- readr::read_rds(
  file = here::here("simulation-studies", "outcome-distribution.rds")
)
```

## Simulation Setting

## Technical Aspects

As outlined in the previous paragraph, a total of $50 \cdot 3 \cdot 3 = 450$ models were fitted to analyze the performance differences.
In order to speed up the involved computations of this specific and some of the other simulation studies illustrated in this report, we used the *parallel computing* capabilities of `R`.

There are many options from various packages to choose from.
We decided to use the [furrr](https://furrr.futureverse.org/) package which is built on top of the `future` package specialized on parallel processing.
As the name suggests, `furrr` provides a convenient way to use many functions from the popular `purrr` package while using multiple cores at the same time. 
This *functional programming* based approach (similar to the `apply()` family in 'base R') is particularly well suited for simulation studies and provides some structural as well as minor performance advantages compared to the classical `for`-loop approach, which, however, are beyond the scope of this report.

The following (slightly modified) code snippet provides a brief insight into the implementation:

```{r, eval=FALSE}
plan(multisession, workers = 8)

full_results <- tibble(id = 1:50) %>%
  mutate(samples = future_map(
    .x = id,
    .f = ~ show_results(n = 50, num_sim = 1000),
    .options = furrr_options(seed = 1)
  ))
```

The `plan()` function borrowed from the `future` package initializes the parallel computing process and the number of cores available for computation.
The `show_results()` helper function fits all three models `mcmc_ridge()`, `mcmc()` and `lmls()` for each outcome distribution in a single iteration.
This procedure is repeated $50$ times in parallel using the `future_map()` function from the `furrr` package, where the results of all $450$ models are saved in a well organized structure inside of a list column. 
This new column of the data frame contains complete information about all simulations such that any required element for the further analysis can be easily extracted and post processed.

Finally, the `.options()` argument allows the specification of a random seed.
Random number generation in the context of parallel computing is slightly more involved than in the sequential approach.
This additional complexity is automatically handled by the `future_map()` function, such that all results are sampled in a statistically valid and fully reproducible manner.

## Simulation Results

```{r, echo=FALSE, out.width="100%"}
outcome_dist_list$plot_single_sim
```

```{r, echo=FALSE, out.width="100%"}
outcome_dist_list$plot_many_sims
```




```{r, echo=FALSE}
outcome_dist_list$data_many_sims %>%
  filter(Parameter %in% c("beta_0", "beta_2", "beta_4", "gamma_0", "gamma_2")) %>%
  select(-contains("se")) %>%
  pivot_longer(cols = contains("bias")) %>%
  pivot_wider(names_from = c(outcome_dist, name), values_from = value) %>%
  mutate(Parameter = stringr::str_glue("$\\{Parameter}$")) %>%
  tibble::column_to_rownames(var = "Parameter") %>%
  magrittr::set_colnames(rep(c("lmls", "mcmc", "mcmc\\_ridge"), 3)) %>%
  kableExtra::kbl(
    digits = 2, align = "c", booktabs = TRUE, escape = FALSE,
    caption = "Bias of Coefficient Estimates"
  ) %>%
  kableExtra::kable_styling(
    latex_options = "HOLD_position", position = "center", full_width = FALSE
  ) %>%
  kableExtra::add_header_above(
    header = c("", "Normal" = 3, "t" = 3, "Uniform" = 3)
  )
```

```{r, echo=FALSE}
outcome_dist_list$data_many_sims %>%
  filter(Parameter %in% c("beta_0", "beta_2", "beta_4", "gamma_0", "gamma_2")) %>%
  select(-contains("bias")) %>%
  pivot_longer(cols = contains("se")) %>%
  pivot_wider(names_from = c(outcome_dist, name), values_from = value) %>%
  mutate(Parameter = stringr::str_glue("$\\{Parameter}$")) %>%
  tibble::column_to_rownames(var = "Parameter") %>%
  magrittr::set_colnames(rep(c("lmls", "mcmc", "mcmc\\_ridge"), 3)) %>%
  kableExtra::kbl(
    digits = 2, align = "c", booktabs = TRUE, escape = FALSE,
    caption = "Standard Errors of Coefficient Estimates"
  ) %>%
  kableExtra::kable_styling(
    latex_options = "HOLD_position", position = "center", full_width = FALSE
  ) %>%
  kableExtra::add_header_above(
    header = c("", "Normal" = 3, "t" = 3, "Uniform" = 3), escape = FALSE
  )
```



# Sample Size

```{r, include=FALSE}
plot_data <- readr::read_rds(
  file = here::here("simulation-studies", "samplesize_1.rds")
)
```

```{r, echo=FALSE, out.width="100%"}
mean_mean <- plot_data$mean_mean
n_data <- plot_data$n_data

matplot(t(mean_mean),
  x = n_data, type = "b", pch = 1, col = 1:6,
  main = " Mean of Posterior Means",
  xlab = "Sample size", ylab = ""
)
legend(
  x = "topright", legend = rownames(mean_mean),
  col = 1:6, pch = 1, bty = "n"
)
```

```{r, echo=FALSE, out.width="100%"}
mean_absolute_error <- plot_data$mean_absolute_error

matplot(t(mean_absolute_error),
  x = n_data, type = "b", pch = 1, col = 1:6,
  main = " MAE of Posterior Means",
  xlab = "Sample size", ylab = ""
)
legend(
  x = "topright", legend = rownames(mean_absolute_error),
  col = 1:6, pch = 1, bty = "n"
)
```

```{r, echo=FALSE, out.width="100%"}
mean_squared_error <- plot_data$mean_squared_error

matplot(t(mean_squared_error),
  x = n_data, type = "b", pch = 1, col = 1:6,
  main = " MSE of Posterior Means",
  xlab = "Sample size", ylab = ""
)
legend(
  x = "topright", legend = rownames(mean_squared_error),
  col = 1:6, pch = 1, bty = "n"
)
```

# Number of Simulations

```{r, include=FALSE}
num_sim_data <- readr::read_rds(
  file = here::here("simulation-studies", "number_mcmc_ridge_simulations.rds")
)
```

```{r, echo=FALSE, out.width="100%"}
mean_mean_num_sim <- num_sim_data$mean_mean
array_num_sim <- num_sim_data$array_num_sim

matplot(t(mean_mean_num_sim),
  x = array_num_sim, type = "b", pch = 1, col = 1:6,
  main = " Mean of Posterior Means",
  xlab = "Number of MCMC Ridge Simulations", ylab = ""
)
legend(
  x = "topright", legend = rownames(mean_mean_num_sim),
  col = 1:6, pch = 1, bty = "n"
)
```

```{r, echo=FALSE, out.width="100%"}
mean_absolute_error_num_sim <- num_sim_data$mean_absolute_error
array_num_sim <- num_sim_data$array_num_sim

matplot(t(mean_absolute_error_num_sim),
  x = array_num_sim, type = "b", pch = 1, col = 1:6,
  main = " MAE of Posterior Means",
  xlab = "Number of MCMC Ridge Simulations", ylab = ""
)
legend(
  x = "topright", legend = rownames(mean_absolute_error_num_sim),
  col = 1:6, pch = 1, bty = "n"
)
```

```{r, echo=FALSE, out.width="100%"}
mean_squared_error_num_sim <- num_sim_data$mean_squared_error
array_num_sim <- num_sim_data$array_num_sim

matplot(t(mean_squared_error_num_sim),
  x = array_num_sim, type = "b", pch = 1, col = 1:6,
  main = " MSE of Posterior Means",
  xlab = "Number of MCMC Ridge Simulations", ylab = ""
)
legend(
  x = "topright", legend = rownames(mean_squared_error_num_sim),
  col = 1:6, pch = 1, bty = "n"
)
```

```{r, echo=FALSE, out.width="100%"}
mean_of_variances_within <- num_sim_data$mean_of_variances_within
array_num_sim <- num_sim_data$array_num_sim

matplot(t(mean_of_variances_within),
  x = array_num_sim, type = "b", pch = 1, col = 1:6,
  main = " Mean of Variances within the Samples",
  xlab = "Number of MCMC Ridge Simulations", ylab = ""
)
legend(
  x = "topright", legend = rownames(mean_of_variances_within),
  col = 1:6, pch = 1, bty = "n"
)
```



# Hyperparameters



# Next Steps {-}

In the process of writing the first and second report, the main work of developing both the mathematical foundation as well as the code base of the `asp21bridge` package has been done.
Thus, the following weeks will focus on filling remaining gaps and working out details that were left open due to time constraints. 
This includes taking a closer look at code efficiency and possible code refactoring for the sake of modularity.

Further, we will reach out to other groups with similar topics (the Bayesian Lasso Regression group in particular) in order to initiate a collaboration. 
In addition to exploring similarities and differences between our own sampler and the procedures from the `lmls` package, this would allow for an interesting comparison between different penalization approaches.

The final step consists of combining all pieces of the project into a single structured and consistent final report.
Aside of the work shown in the first two reports, some new elements like a brief discussion of design and implementation choices during the development stage will be added with the aim of providing the reader a solid understanding of the underlying ideas as well as the practical usage of the package.
