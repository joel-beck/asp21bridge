---
output:
  github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# asp21bridge

The goal of `asp21bridge` is to extend the [lmls](https://gitlab.gwdg.de/asp21/lmls) package by implementing a Markov Chain Monte Carlo Sampler with Ridge penalization.

## Underlying Model

The observation model in consideration is given by
<!--  -->
```math
\begin{aligned}
y_i \sim \mathcal{N} \left( \mathbf{x}_i^T \boldsymbol{\beta},\, \exp \left( \mathbf{ z}_i^T \boldsymbol{\gamma} \right)^2 \right) \qquad i = 1, \ldots n,
\end{aligned}
```
<!--  -->
where the *location* parameter $\beta$ and the *scale* parameter $\gamma$ are themselves normally distributed with prior mean $0$ and *hyperparameters* $\tau^2$ and $\xi^2$ for the prior variances.

## The Data

In the following examples the built-in simulated `toy_data` is used, which consists of a column `y` representing a vector of observed values and the explanatory variables `x1`, `x2`, `z1` and `z2`.
The data is constructed according to the mathematical model in the previous section,
where all explanatory variables predict the mean of `y` and only the latter two model the variance.

```{r}
library(asp21bridge)
head(toy_data)
```

## Sampling Process

We first fit the frequentist `lmls` regression model and extend this approach by adding the MCMC samples of the posterior distributions.
For later use, we also apply the MCMC sampler without Ridge - regularization from the `lmls` package:

```{r}
set.seed(1234)

fit <- toy_data %>%
  lmls(location = y ~ ., scale = ~ z1 + z2, light = FALSE) %>%
  mcmc(nsim = 1000) %>%
  mcmc_ridge(num_sim = 1000)
```

## Numerical Analysis

The `asp21bridge` package contains various tools to analyze the sampling results both numerically and graphically.

A first quick overview can be gained by the generic `summary()` function with specification of the `type` argument:

```{r}
summary(fit, type = "mcmc_ridge")
```

A more comprehensive list of the estimated parameter values is provided by the `summary_complete()` function:

```{r}
summary_complete(fit)
```

Since the results are embedded in a `data frame`, the usual methods of data frame manipulation allow for a convenient analysis even for high dimensional parameter vectors.
A particularly interesting use case is the comparison of the coefficient estimates from the following three approaches:

- Point estimates based on Maximum Likelihood Estimation (`mle` column)

- Posterior Mean estimates from MCMC sampling without regularization (`mcmc` column)

- Posterior Mean estimates with Ridge penalty (`mcmc_ridge` column)

Both the `mcmc()` function as well as the `mcmc_ridge()` function simply add their results to the existing model.
Therefore all relevant information is contained in the `fit` object from above:

```{r}
summary_complete(fit) %>%
  dplyr::transmute(Parameter, mcmc_ridge = `Posterior Mean`) %>%
  dplyr::filter(stringr::str_detect(Parameter, pattern = "beta|gamma")) %>%
  dplyr::mutate(
    mcmc = summary_complete(fit$mcmc)$`Posterior Mean`,
    mle = c(coef(fit)$location, coef(fit)$scale),
    truth = c(0, -2, -1, 1, 2, 0, -1, 1)
  )
```

The estimates for $\beta_1$ up to $\beta_4$ are identical in the three models and almost coincide with the true data generating values.
This behaviour is somewhat expected, since the $\beta$ vector is drawn from a closed form multivariate normal distribution with independent samples across iterations of the MCMC sampler.

The results for the scale parameter $\boldsymbol{\gamma}$ show a greater variation between the models and are therefore more interesting to analyze.
The Maximum Likelihood approach is able to recover the true / data generating value of $\gamma_2$, while the MCMC sampler with Ridge penalty performs best for estimating $\gamma_1$.
All three models fail to identify the correct intercept parameter $\gamma_0$, which, however, is often of minor interest.

Simulations across different data sets and sample sizes show similar patterns:
If the data is generated according to the mathematical model introduced in the first section, i.e. the estimation model using Maximum Likelihood is correctly specified, the `mle` and the `mcmc_ridge` estimates tend to be closest to the true values.
If, however, the underlying model used for simulation deviates from the Gaussian location-scale regression model, the `mle` results are, unsurprisingly, worse than both of the more flexible MCMC methods.

The Markov Chain Monte Carlo samples for $\boldsymbol{\gamma}$ based on the Metropolis - Hastings algorithm implemented in the `mcmc_ridge()` function are strongly correlated, even though the acceptance rate of the proposed values is reasonable:

```{r}
fit$mcmc_ridge$acceptance_rate
```

This value of roughly `r 100 * round(fit$mcmc_ridge$acceptance_rate, 2)`% indicates that the variance of the proposal distribution is chosen appropriately.

## Graphical Analysis

The building blocks for monitoring the convergence of the posterior chains as well as the autocorrelations are the functions `diagnostic_plots()` for a single Markov Chain and `mult_plot()` for combining multiple chains.

A quick overview can be gained by collecting the corresponding trace plots for all posterior coefficients.

```{r}
mult_plot(fit, type = "trace", free_scale = TRUE, latex = TRUE)
```

The trace plots for the $\boldsymbol{\beta}$ coefficients indicate convergence and confirm the stable posterior estimates from the previous section.

The prior variance parameters $\tau^2$ and $\xi^2$ are strictly positive and thus more adequately displayed on the logarithmic scale:

```{r}
samples <- fit$mcmc_ridge$sampling_matrices
mult_plot(
  samples = list(samples$location_prior, samples$scale_prior),
  type = "both", log = TRUE, latex = TRUE
)
```

Just like the $\boldsymbol{\beta}$ samples, these are drawn from closed form full conditional distributions.

Finally, we focus on the samples for the scale parameter $\gamma$.
Here, the trace plots do not indicate convergence to the posterior distribution.
Thus, we increase the number of simulations to $10000$, which also allows the inclusion of a Burn - In Phase as well as a thinning factor while still maintaining a sufficient sample size to estimate posterior quantities:

```{r}
set.seed(4321)
fit <- fit %>%
  mcmc_ridge(num_sim = 10000)
```

We drop the first $1000$ samples for $\gamma_2$ and show the three most common diagnostic plots for the thinned sample:

```{r}
samples <- fit$mcmc_ridge$sampling_matrices

samples$scale[, 3, drop = FALSE] %>%
  burnin(num_burn = 1000) %>%
  thinning(freq = 30) %>%
  diagnostic_plots(lag_max = 30, latex = TRUE)
```

Even a thinning factor of $30$, i.e. only every $30$th observation is kept in the sample, does not get rid of the autocorrelation.
Yet, the posterior density seems to be approximately normal and the trace plot indicates a decent exploration of the posterior space.

These findings serve as a great starting point for a more in-depth analysis, which is beyond the scope of this brief introduction to the `asp21bridge` package.
Full descriptions of all functions, their arguments and common applications can be found in the documentation.
