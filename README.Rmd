---
output:
  github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# asp21bridge 

The goal of `asp21bridge` is to extend the `lslm` package by implementing a Markov Chain Monte Carlo Sampler with Ridge penalization. 

## Underlying Model 

The observation model in consideration is given by
<!--  -->
```math
\begin{aligned}
y_i \sim \mathcal{N} \left( \mathbf{x}_i^T \boldsymbol{\beta},\, \exp \left( \mathbf{ z}_i^T \boldsymbol{\gamma} \right)^2 \right) \qquad i = 1, \ldots n,
\end{aligned}
```
<!--  -->
where the *location* parameter $\beta$ and the *scale* parameter $\gamma$ are themselves normally distributed with prior mean $0$ and *hyperparameters* $\tau^2$ and $\xi^2$ for the prior variances.

## The Data

In the following examples the built-in simulated `toy_data` is used, which consists of a column `y` representing a vector of observed values and the explanatory variables `x1`, `x2`, `z1` and `z2`.
Here, all explanatory variables predict the mean of `y` whereas only the latter two model the variance.

```{r}
library(asp21bridge)
head(toy_data)
```

## Sampling Process

We first fit the frequentist `lslm` regression model and extend this approach by adding the MCMC samples of the posterior distributions.
For later use, we also apply the MCMC sampler without Ridge - regularization from the `lslm` package:

```{r}
set.seed(1234)

fit <- toy_data %>%
  lslm(location = y ~ ., scale = ~ z1 + z2, light = FALSE) %>%
  mcmc(nsim = 1000) %>%
  gibbs_sampler(num_sim = 1000)
```

## Numerical Analysis

The `asp21bridge` package contains various tools to analyze the sampling results both numerically and graphically.

A first quick overview can be gained by the generic `summary()` function with specification of the `type` argument:

```{r}
summary(fit, type = "mcmc_ridge")
```

A more comprehensive list of the estimated parameter values is provided by the `summary_complete()` function:

```{r}
summary_complete(fit)
```

Since the results are embedded in a `data frame`, the usual methods of data frame manipulation allow for a convenient analysis even for high dimensional parameter vectors.
A particularly interesting use case is the comparison of the coefficient estimates from the following three approaches:

- Point estimates based on Maximum Likelihood Estimation (`mle` column)

- Posterior Mean estimates from MCMC sampling without regularization (`mcmc` column)

- Posterior Mean estimates with Ridge penalty (`mcmc_ridge` column)

Both the `mcmc()` function as well as the `gibbs_sampler()` function simply add their results to the existing model. 
Therefore all relevant information is contained in the `fit` object from above:

```{r}
summary_complete(fit) %>%
  dplyr::transmute(Parameter, mcmc_ridge = `Posterior Mean`) %>%
  dplyr::filter(stringr::str_detect(Parameter, pattern = "beta|gamma")) %>%
  dplyr::mutate(
    mcmc = summary_complete(fit$mcmc)$`Posterior Mean`,
    mle = c(coef(fit)$location, coef(fit)$scale)
  )
```

The estimates for $\beta_1$ up to $\beta_4$ are identical in the three models and coincide with the true data generating values.
This behaviour is somewhat expected since the $\beta$ vector is drawn from a closed form multivariate normal distribution with independent samples across iterations of the MCMC sampler.

The results for the scale parameter $\gamma$ show a greater variation between the models and are therefore more interesting to analyze.
The Markov Chain Monte Carlo samples based on the Metropolis - Hastings algorithm implemented in the `gibbs_sampler()` function are strongly correlated, even though the acceptance rate of the proposed values is reasonable:

```{r}
fit$mcmc_ridge$acceptance_rate
```

This value of roughly `r 100 * round(fit$mcmc_ridge$acceptance_rate, 2)`% indicates that the variance of the proposal distribution is chosen appropriately. 

## Graphical Analysis

The building blocks for monitoring the convergence of the posterior chains as well as the autocorrelations are the functions `diagnostic_plots()` for a single Markov Chain and `mult_plot()` for combining multiple chains.

A quick overview can be gained by collecting the corresponding time plots for all posterior coefficients.

```{r}
mult_plot(fit, type = "time", free_scale = TRUE, latex = TRUE, robust = TRUE)
```

The time plots for the $\beta$ coefficients indicate convergence and confirm the stable posterior estimates from the previous section.

The prior variance parameters $\tau^2$ and $\xi^2$ are strictly positive and thus more adequately displayed on the logarithmic scale:

```{r}
samples <- fit$mcmc_ridge$sampling_matrices
mult_plot(
  samples = list(samples$location_prior, samples$scale_prior),
  type = "both", log = TRUE, latex = TRUE
)
```

Just like the $\beta$ samples, these are drawn from closed form full conditional distributions.

Finally, we focus on the samples for the scale parameter $\gamma$.
Here, the time plots do not indicate convergence to the posterior distribution.
Thus, we increase the number of simulations to $10000$, which also allows the inclusion of a Burn - In Phase as well as a thinning factor while still maintaining a sufficient sample size to estimate posterior quantities:

```{r}
set.seed(4321)

fit <- fit %>% 
  gibbs_sampler(num_sim = 10000)
```

We drop the first $1000$ samples for $\gamma_2$ and show the three most common diagnostic plots for the thinned sample:

```{r}
samples <- fit$mcmc_ridge$sampling_matrices

samples$scale[, 3, drop = FALSE] %>%
  burnin(num_burn = 1000) %>% 
  thinning(freq = 30) %>%
  diagnostic_plots(lag_max = 30, latex = TRUE)
```

Even a thinning factor of $30$, i.e. only every $30$th observation is kept in the sample, does not get rid of the autocorrelation. 
The posterior density seems to be approximately normal though and the time plot indicates a decent exploration of the posterior space.

These findings serve as a great starting point for a more in-depth analysis, which is beyond the scope of this brief introduction to the `asp21bridge` package.
Full descriptions of all functions, their arguments and common applications can be found in the documentation.
